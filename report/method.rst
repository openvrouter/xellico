
.. _method:

研究の方法
==========

ここでは, 本研究の進め方を説明していく.
今後, 根本的なアーキテクチャの変更がなく,
より高性能をサポートするハードウェアが現れた場合でも,
本研究の手法を同じく容易に適用できるように研究方法を設定した.

まず最初に, 以下に示す環境を前提として研究を進めていく.

- OnlyCOTS: 一般的なIAアーキテクチャを採用し,
  FPGA等の特別なハードウェアをパケット処理に使用しない.
  NICはPCIeにより接続され, DPDKから利用可能な状態である.
  HW支援機能としてRSSが利用可能状態である.
- NUMA Aware: 複数プロセッサマシンを利用する

VNFの簡単な性能推定の練習
-------------------------

例えば0番ポートから受信したパケットを1番ポートに転送するアプリケーションを
考えて見る. 多くのDPDKアプリケーションでは以下に示す様な手順でパケット処理を行う.

.. code-block:: none

  pkts, n_rx = rx(portid=0, bulk=32);
  for (i=0; i<n_rx; i++) {
    process_packet(packet=pkts[i]);
    tx(portid=1, packet_array=&pkts[i], n_tx=1);
  }

しかし, DPDKでは, TxBufferを用いることにより, 一定にバルクサイズに
到達するまで送信処理を行わない手法が取られる. [DPDKl3fwddoc]_, [DPDKl3fwdsrc]_
その様にVNFを実装した場合, 計算の構造は以下に示すような状態として
考えることが可能である.  これは常にパケットが着続けている状態では
以下のコードと上で示したコードの二つが同じ計算を行うと言うことを示している.

.. code-block:: none

  pkts, n_rx = rx(portid=0, bulk=32);
  for (i=0; i<n_rx; i++) {
    process_packet(packet=pkts[i]);
  }
  tx(portid=1, packet_array=&pkts[i], 32);

本研究では, 上記のコードは同じ性能を示すという前提を固定して研究を進める.

このアプリケーションの性能目標を定めた場合, 上記の各それぞれはどの様な
性能を達成する必要があるだろうか. 例えば, 上記のコードの ``rx()`` ``tx()``
は, 一度の実行ごとに, 100 clockの計算時間を必要とし, ``process_packet()``
は, 一度の実行ごとに, 200 clockの計算時間が必要だとしよう.
この時のVNFの性能は,
``100 + 32 * 200 + 100 = 6600clock/32packet, 206clock/packet``
となる. 3GHzのCPUを用いた場合, 200 clockにかかる時間は, ``66*10^(-9) sec``
すなわち, 66nsec/packetとなる. 10GbEにおける64Byteワイヤーレートは14Mpps,
``約67nsec/packet`` なため, ``66nsec/packet`` なら10GbEの64Bワイヤーレート
を達成できそうである.

汎用の計算機を用いたVNF開発における性能推定の計算は上記の様に行うが,
実際にその様な単純な計算のみで全てことたりるだろうか. 答えはNoである.
例えば ``rx()`` のblukサイズを1とした場合と32とした場合では関数の
RTT(Round Trip Time)に違いが生じるのは明らかであろう.
また, ``rx()``, ``tx()`` 内部ではPCIe NICのデバイスドライバのコードが実行される.
NICにはPCIeを介してアクセスするため, トランザクションはできるだけまとめた方が
効率的かもしれない. また, RSSを利用した場合, NICに対する単位時間のアクセスが
コア数が増える分増加する事も考えられる. NICに対するアクセスが増加した場合,
PCIeのバスを圧迫して性能低下が起きる可能性はあるのか.
そして, これらは全てのNICに共通した性能をだすのだろうか.おそらく,
これらの値はNICの種類によって違いが生じるだろう.

ここまで説明しただけでもわからないことがたくさん出てきた.
本研究ではそれらの疑問を細かいマイクロベンチマークを測ることにより明らかに
していく.

まず, 先ほどの様な (擬似コードの例) DPDKのシンプルなアプリケーションの
関数レイテンシを測る方法を定義し, これを用いてDPDKのAPIの細かい関数遅延,
(RTTともよぶ)を計測する. 次に, これらで求めた値が, RSS等のHW支援機能を
利用した時に性能に変化があるかを検討する. 今回調べるHW支援機能はRSSのみを
対象とするが, 同様の手順で他のHW支援機能も計測することが可能である.
(例えばChecksum計算OffloadやFlow Director等) しかし, パケットのドロップを
伴う場合は, パケット処理のpathが先ほど示したモデルと変わるため, 再検討が
必要である.

本研究では,これらの性能計測を円滑に行うために,
Xellicoというソフトウェアネットワークアプライアンスを新規に開発し,
計測を行なった. Xellicoはオープンソースで開発を行なっており,
DPDKのexample/l3fwdをforkして開発開始を行なった.
本研究で調節を行たい, RSSとTxBufferの構成パターンを実行時に
jsonファイルから入力することができ,性能評価に適している.

本研究の性能計測は,2台のXeonサーバを用いて行なった, 片方でDUTとして
Xellicoを動作させ, 他方では通信性能計測のためにpktgen-dpdkを動作させた.
性能は帯域と遅延に関して行う.  また, 性能計測結果の正確な考察を行うために,
TimeStampClockを用いてDPDKの関数のマイクロレイテンシを計測した.

研究の目標
-----------

先ほどの方針から, 研究の目標を以下の2つとして定める.

- ハードウェア特性をまとめる
- VNF開発時に性能目標と使用HWを決めた時の細かい目標を単純に示す.

ハードウェア特性をまとめる
--------------------------

HW特性とはなにか. 例えばよく知られているHW特性を考えてみよう.
Intel XL710は64Byteのパケットではワイヤーレートで動作をしない.
[PANDA20150409]_ , [YASUaintec]_ . 他にも, Intel x540はNUMAまたぎの
RSS queueを1個にした時, パケットフォワード時に8500Mbps近くを天井に性能限界に陥る.
これらはソフトウェア側からは判断仕切れない問題であり, 現状では
どこに問題があるのかが推定しきれない.
しかし, 細かくHWカウンタ等を記録し, それを外部トラフィック計測器と
照らし合わせることにより, どこでパケットが送信仕切れていないのか. 等が
わかるはずである.

本研究では, 内部で計測したマイクロレイテンシと, 外部で計測したベンチマークを
照らし合わせ, どのレイヤでこれらのギャップが生じているかを推定する.

VNF開発時に性能目標と使用HWを決めた時の細かい目標を単純に示す
--------------------------------------------------------------

本研究の成果として, 新しいVNFを開発する時の性能推定があげられる.
実際に使用するハードウェアの細かいマイクロベンチマークを集めて
計算することにより, 性能を実際に図らずとも推定することをしたい.
例えば, 3GHzで動くXeon E5の2socket サーバを容易し, 各socketに
2つずつ, Intel X540 1portを装着した場合での細かいマイクロベンチマーク
はどの様になるのか. ``rte_eth_rx_brust`` 関数の遅延はburstサイズに
合わせてどの様に変化するのか. 等を調べる必要があある.

本研究の実験の流れ
------------------

- 実験準備

  - 測る方法を決める (BCC/TCS)
  - 決めた方法で測る
  - 測った値が正しいかを調べる
  - 外部, 内部のギャップを示す.
  - ギャップが存在する場合の理由の仮説をたて, 仮説を検証する

- 準備した方法で,RSS, TxBufferの性能特性を調べる
- 一般的なVNF開発のための性能推定方法を式で示す.


.. toctree::
  :maxdepth: 1

  method_environment
  method_xellico
  method_eval


